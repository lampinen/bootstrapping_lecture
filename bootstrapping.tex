\documentclass{beamer} %use [handout] to get summary]
\usepackage{pgfpages}
%\setbeameroption{show notes on second screen=left} %enable for notes
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{hyperref}
\lstset{language=R,frame=single}
\usepackage{verbatim}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{relsize}
\usepackage{appendixnumberbeamer}
\usepackage{xparse}
\usepackage{multimedia}
\usepackage{tikz}
\usetikzlibrary{matrix,backgrounds}
\pgfdeclarelayer{myback}
\pgfsetlayers{myback,background,main}

\tikzset{mycolor/.style = {line width=1bp,color=#1}}%
\tikzset{myfillcolor/.style = {draw,fill=#1}}%
\tikzstyle{line} = [draw, line width=1pt]
\tikzstyle{arrow} = [draw, line width=1pt, ->]

\NewDocumentCommand{\highlight}{O{blue!40} m m}{%
\draw[mycolor=#1,rounded corners] (#2.north west)rectangle (#3.south east);
}

\NewDocumentCommand{\fhighlight}{O{blue!40} m m}{%
\draw[myfillcolor=#1,rounded corners] (#2.north west)rectangle (#3.south east);
}

\usetheme[numbering=fraction]{metropolis}
%%\AtBeginSection[]
%%{
%%  \begin{frame}
%%    \frametitle{Table of Contents}
%%    \tableofcontents[currentsection]
%%  \end{frame}
%%}

%%\let\olditem\item
%%\renewcommand{\item}{\vspace{0.5\baselineskip}\olditem}

\newcommand{\E}[1]{\mathbb{E}\left[#1\right]}
\begin{document}

\title{Bootstrapping}
\subtitle{(and a bit more)}
\author{Andrew Lampinen}
\date{Psych 252, Winter 2019}
\frame{\titlepage}


\section{Introduction}

\begin{frame}{Outline}
\vspace{1em}
\tableofcontents
\end{frame}


\section{Why bootstrapping?}
\begin{frame}[standout]
Bootstrapping makes fewer assumptions than parametric methods. 
\end{frame}

\begin{frame}[label=heteroscedasticity]
\frametitle{Non-IID noise (heteroscedasticity)}
\only<6-7>{
Now assume the null:
}
\only<-7>{
\begin{figure}
\centering
\only<1>{
\includegraphics[width=\textwidth]{figures/error_dist_non_null.png}
}
\only<2>{
\includegraphics[width=\textwidth]{figures/heteroscedasticity_consequences.png}
}
\only<3>{
\includegraphics[width=\textwidth]{figures/what_does_that_mean.png}
}
\only<4>{
\includegraphics[width=\textwidth]{figures/error_dist_CI_example_parametric_only.png}
}
\only<5>{
\includegraphics[width=\textwidth]{figures/error_dist_CI_example.png}
}
\only<6>{
\includegraphics[width=\textwidth]{figures/error_dist_null.png}
}
\only<7>{
\includegraphics[width=\textwidth]{figures/error_dist_proportion_significant.png}
}
\end{figure}
}
\only<8->{
    \begin{itemize}[<+(7)->] \itemsep 1em
    \item Linear models assume IID noise.
    \item Therefore heteroscedasticity leads to incorrect confidence intervals \textbf{and} hypothesis tests.
    \item Bootstrapping avoids this assumption, and so can still behave appropriately.
    \end{itemize}
}
\end{frame}

\begin{frame}[standout]
Bootstrapping makes fewer assumptions than parametric methods, so it works better when those assumptions don't hold. 
\end{frame}

\begin{frame}{A case study in weird tests}
\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{figures/conceptual_density_plot.png}
\end{figure}
$$\text{Test statistic} = \frac{D_\text{KL}\left(P_1 || \frac{P_1 + P_2}{2}\right) + D_\text{KL}\left(P_2 || \frac{P_1 + P_2}{2}\right)}{2}$$
\end{frame}

\begin{frame}[standout]
Bootstrapping makes fewer assumptions than parametric methods, so it can be applied in cases where parametric sampling distributions aren't known. 
\end{frame}

\section{What is bootstrapping anyway?}

\begin{frame}[standout]
Fundamental idea: Your best (or only) estimate of what's happening in the population is what's happening in your sample. 
\end{frame}

\againframe<1>{heteroscedasticity}

\begin{frame}[standout]
So instead of making assumptions about the population, we'll use what we actually know from the sample.
\end{frame}

\begin{frame}{Bootstrap resampling}
\begin{itemize}[<+->] \itemsep 1em
\item We want to understand our uncertainty about a statistic we are estimating (e.g. a difference of means).
\item One way to do this would be to run many experiments, and see how much that parameter varies, but usually this is infeasible.
\item Instead, we assume our sample distribution closely approximates the population distribution.
\item Therefore we can \textbf{simulate} these repeated experiments by \textbf{resampling from our sample with replacement.}
\end{itemize}
\end{frame}

\begin{frame}{Bootstrap resampling demo}
\begin{figure}
\centering
\only<1>{
\includegraphics[width=\textwidth]{figures/bootstrap_demo_0.png}
}
\only<2>{
\includegraphics[width=\textwidth]{figures/bootstrap_demo_1.png}
}
\only<3>{
\includegraphics[width=\textwidth]{figures/bootstrap_demo_2.png}
}
\only<4>{
\includegraphics[width=\textwidth]{figures/bootstrap_demo_3.png}
}
\only<5>{
\includegraphics[width=\textwidth]{figures/bootstrap_demo_4.png}
}
\only<6>{
\includegraphics[width=\textwidth]{figures/bootstrap_demo_5.png}
}
\end{figure}
\end{frame}

\section{Bootstrap confidence intervals}

\begin{frame}{Bootstrap confidence intervals}
A simple application:
\begin{itemize}[<+(1)->] \itemsep 1em
\item We compute a statistic (such as the mean) from some data, and want to understand our uncertainty about it.
\item Remember: The bootstrap idea is that we can \textbf{simulate} these repeated experiments by \textbf{resampling from our sample with replacement.}
\item Now, we'll use the sampling distribution that we ``observe'' in these ``experiments'' to construct a confidence interval.
\item The simplest method (``percentile'') just uses the 2.5\% and 97.5\% quantiles of the bootstrap sampling distribution as the CI endpoints.
\end{itemize}
\end{frame}

\begin{frame}<1>[fragile, label=boot_CI_demo]
\frametitle{Bootstrap CI demo}
\begin{figure}
\centering
\only<1>{
\includegraphics[width=\textwidth]{figures/bootstrap_CI_0.png}
}
\only<2>{
\includegraphics[width=\textwidth]{figures/bootstrap_CI_1.png}
}
\only<3>{
\includegraphics[width=\textwidth]{figures/bootstrap_CI_2.png}
}
\end{figure}
\end{frame}

\begin{frame}[standout]
To compute a bootstrap percentile CI, compute a bootstrap sampling distribution and take the 2.5\% and 97.5\% quantiles as the CI endpoints.
\end{frame}

\againframe<2->{boot_CI_demo}

\begin{frame}[standout]
Generally BCA intervals are superior. You can get them from the boot library in R!
\end{frame}

\section{Bootstrap (and permutation) tests}

\begin{frame}[standout]
Estimation is at least as important as testing! 
\end{frame}

\begin{frame}{Bootstrap hypothesis tests}
How can we use bootstrapping for hypothesis testing?
\begin{itemize}[<+(1)->]
\item Construct a bootstrap $1-\alpha$ confidence interval.
\item Reject the null if the null hypothesis value of the statistic isn't included in this CI.
\end{itemize}
\uncover<4-5>{
\includegraphics[width=\textwidth]{figures/bootstrap_test.png}
\begin{tikzpicture}[overlay, remember picture]
\only<4>{
\fill [bg] (5.25, 0) -- (5.25, 6) -- (10.8, 6) -- (10.8, 0);
}
\end{tikzpicture}
}
\end{frame}

\begin{frame}[standout]
Bootstrap hypothesis testing: reject the null if the null value isn't contained in the CI.
\end{frame}

\begin{frame}{Bootstrap tests \& $p$-values: some food for thought}
There are some subtle conceptual issues here:
\begin{itemize}[<+(1)->]
\item In NHST, we calculate the sampling distribution \textbf{under the null}, and see whether \textbf{our observed statistic} is surprising.
\item Here, we calculate the (approximate) sampling distribution under the \textbf{(approximate) population distribution,} and see whether the \textbf{null value} is surprising.
\item In many cases (such as anytime the null is true, or if the assumptions of e.g. linear regression hold), bootstrap tests are \textbf{exactly} NHST.
\item More generally, the bootstrap testing procedure \textbf{is valid}, in the sense that it has the nominal false-positive rate, but the interpretation of the $p$-value is not the same as in NHST.  
\end{itemize}
\end{frame}

\begin{frame}{Permutation tests}
Sometimes we can actually nonparametrically sample from the null.
\end{frame}


\section{Bootstrap power analyses}


\section{Wrapping up}

\begin{frame}{Summary}

\end{frame}

\begin{frame}{Further reading}
\begin{itemize}
\item The classic:
    \begin{itemize}
    \item An Introduction to the Bootstrap, Efron \& Tibshirani, 1993
    \end{itemize}
\item Bootstrap hypothesis testing 
    \begin{itemize}
    \item \url{https://core.ac.uk/download/pdf/6494364.pdf} 
    \end{itemize}
\item More advanced \& general: 
    \begin{itemize}
    \item All of Nonparametric Statistics, Larry Wasserstein, 2006 (available electronically through Stanford!)
    \item 
    \end{itemize}
\end{itemize}
\end{frame}

\end{document}
