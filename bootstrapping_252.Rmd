---
title: "Bootstrapping Lecture"
author: "Andrew Lampinen"
output:
  html_document:
    toc: true
    toc_float: true
---

```{r}
library(tidyverse)
library(boot)
# devtools::install_github("thomasp85/patchwork", force=T)
library(patchwork) 
```

```{r}
theme_set(theme_classic())
```

# What is bootstrapping anyway?

## What's wrong with parametric tests?

### T-tests on non-normal distributions

Let's see some examples! One common non-normal distribution is the *log-normal* distribution, i.e. a distribution that is normal after you take its logarithm. Many natural processes have distributions like this. One of particular interest to us is reaction times.

```{r}
parametric_plotting_data = data.frame(group = rep(rep(c("Group 1", "Group 2"), each=10000), 2),
                                      type = rep(c("Normal", "Log-normal"), each=20000),
                                      value = c(rnorm(10000, 0, 1),  # normal 1
                                                rnorm(10000, 0.66, 1),  # normal 2
                                                exp(rnorm(10000, 0, 1)),  # Non-normal 1
                                                exp(rnorm(10000, 0.66, 1))))  # Non-normal 2

```

Let's see how violating the assumption of normality changes the results of `t.test`. We'll compare two situations 

Valid: comparing two normally distributed populations with equal means but unequal variances.

Invalid: comparing two log-normally distributed populations with equal means but unequal variances.

```{r}
ggplot(parametric_plotting_data, aes(x=value, color=type)) +
  geom_density() +
  geom_vline(data=parametric_plotting_data %>%
               group_by(group, type) %>%
               summarize(mean_value = mean(value), sd_value = sd(value)),
             aes(xintercept=mean_value, color=type), linetype=2) +
  xlim(-5, 20) +
  facet_grid(group ~ type, scales="free")
```
```{r}
ggsave("figures/log_normal_dists.png", width=5, height=4)
```

```{r}
gen_data_and_test = function(num_observations_per) {
x = rnorm(num_observations_per, 0, 1.1)
y = rnorm(num_observations_per, 1, 1.1)

pnormal = t.test(x, y)$p.value

# what if the data are log-normally distributed?
x = exp(rnorm(num_observations_per, 0, 1.1))
y = exp(rnorm(num_observations_per, 1, 1.1))

pnotnormal = t.test(x, y)$p.value 
return(c(pnormal, pnotnormal))
}

parametric_issues_demo = function(num_tests, num_observations_per) {
  replicate(num_tests, gen_data_and_test(num_observations_per))
}
```

```{r}
set.seed(0) # ensures we get the same results each time we run it

num_tests = 10000 # how many datasets to generate/tests to run
num_observations_per = 20 # how many obsservations in each dataset

parametric_issues_results = parametric_issues_demo(num_tests=num_tests, 
                                                   num_observations_per=num_observations_per)




parametric_issues_d = data.frame(valid_tests = parametric_issues_results[1,],
                                 invalid_tests = parametric_issues_results[2,],
                                 iteration=1:num_tests) %>%
  gather(type, p_value, contains("tests")) %>%
  mutate(is_significant = p_value < 0.05)

# Number significant results with normally distributed data
sum(parametric_issues_results[1,] < 0.05)

# number of significant results with log-normally distributed data
sum(parametric_issues_results[2,] < 0.05)
```
 
```{r}
ggplot(parametric_issues_d, aes(x=type, fill=is_significant)) +
  geom_bar(stat="count", color="black") +
  scale_fill_brewer(palette="Set1") +
  labs(title="Parametric t-test")
```

That's a non-trivial reduction in power from a misspecified model! (~80% to ~54%).

```{r}
boot_mean_diff_test = function(x, y) {
  boot_iterate = function(x, y) {
    x_samp = sample(x, 
                    length(x), 
                    replace=T)
    y_samp = sample(y, 
                    length(y), 
                    replace=T)
    mean_diff = mean(y_samp) - mean(x_samp)
    return(mean_diff)
  }
  boots = replicate(100, boot_iterate(x, y))
  quants = quantile(boots, probs=c(0.025, 0.925))
  return(sign(quants[1]) == sign(quants[2]))

}
```


```{r}
gen_data_and_boot_test = function(num_observations_per) {
x = rnorm(num_observations_per, 0, 1.1)
y = rnorm(num_observations_per, 1, 1.1)

pnormal = boot_mean_diff_test(x, y)

# what if the data are log-normally distributed?
x = exp(rnorm(num_observations_per, 0, 1.1))
y = exp(rnorm(num_observations_per, 1, 1.1))

pnotnormal = boot_mean_diff_test(x, y)
return(c(pnormal, pnotnormal))
}

boot_results = replicate(num_tests, gen_data_and_boot_test(num_observations_per))
sum(boot_results[1,])
sum(boot_results[2,])
```

While the bootstrap **actually loses power** relative to a perfectly specified model, it is much more **robust** to changes in the assumptions of that model, and so it **retains more power when assumptions are violated**.

### Non-IID noise and linear models

```{r}
num_points = 500
true_intercept = 0
true_slope = 1.
set.seed(0)
parametric_ci_data = data.frame(IV = rep(runif(num_points,  -1, 1), 2),
                                type = rep(c("IID Error", "Scaling Error"), each=num_points),
                                error = rep(rnorm(num_points, 0, 1), 2)) %>%
  mutate(DV = ifelse(
    type == "IID Error",
    true_slope*IV + error,
    true_slope*IV + 2*abs(IV)*error)) # error increases proportional to distance from 0 on the IV

```

```{r}
ggplot(parametric_ci_data,
       aes(x=IV, y=DV, color=type)) +
  geom_point(alpha=0.5) +
  geom_smooth(method="lm", se=T, color="black", 
              size=1.5, level=0.9999) + # inflating the confidence bands a bit
                                       # to show their distribution is similar
  scale_color_brewer(palette="Accent") +
  facet_wrap(~type) +
  guides(color=F)
```
```{r}
ggsave("figures/error_dist_non_null.png", width=5, height=3)
```

C.f. Anscombe's quartet, etc.

```{r}
summary(lm(DV ~ IV, parametric_ci_data %>% filter(type=="IID Error")))
summary(lm(DV ~ IV, parametric_ci_data %>% filter(type=="Scaling Error")))
```
```{r}
ex2_lm_bootstrap_CIs = function(data, R=1000) {
  lm_results = summary(lm(DV ~ IV, data=data))$coefficients
  bootstrap_coefficients = function(data, indices) {
    linear_model = lm(DV ~ IV, 
                      data=data[indices,]) # will select a bootstrap sample of the data
    return(linear_model$coefficients)
  }

  boot_results = boot(data=data,
                      statistic=bootstrap_coefficients,
                      R=R)
  boot_intercept_CI = boot.ci(boot_results, index=1, type="bca")
  boot_slope_CI = boot.ci(boot_results, index=2, type="bca")
  return(data.frame(intercept_estimate=lm_results[1, 1],
                    intercept_SE=lm_results[1,2],
                    slope_estimate=lm_results[2, 1],
                    slope_SE=lm_results[2,2],
                    intercept_boot_CI_low=boot_intercept_CI$bca[4],
                    intercept_boot_CI_hi=boot_intercept_CI$bca[5],
                    slope_boot_CI_low=boot_slope_CI$bca[4],
                    slope_boot_CI_hi=boot_slope_CI$bca[5]))
}
```

```{r}
set.seed(0) # for bootstraps
coefficient_CI_data = parametric_ci_data %>%
  group_by(type) %>%
  do(ex2_lm_bootstrap_CIs(.)) %>%
  ungroup() 
```

```{r}
coefficient_CI_data = coefficient_CI_data %>%
  gather(variable, value, -type) %>%
  separate(variable, c("parameter", "measurement"), extra="merge") %>%
  spread(measurement, value) %>%
  mutate(parametric_CI_low=estimate-1.96*SE,
         parametric_CI_hi=estimate+1.96*SE) %>%
  gather(CI_type, value, contains("CI")) %>%
  separate(CI_type, c("CI_type", "high_or_low"), extra="merge") %>%
  spread(high_or_low, value) %>%
  mutate(CI_type = factor(CI_type))
```

```{r}
plot_coefficient_CI_data = function(coefficient_CI_data, errorbar_width=0.5) {
  p = ggplot(data=coefficient_CI_data, aes(x=parameter, color=CI_type, y=estimate, ymin=CI_low, ymax=CI_hi)) +
    geom_hline(data=data.frame(parameter=c("intercept", "slope"),
                               estimate=c(true_intercept, true_slope)), 
               mapping=aes(yintercept=estimate),
               linetype=3) +
    geom_point(size=2, position=position_dodge(width=0.2)) +
    geom_errorbar(position=position_dodge(width=0.2), width=errorbar_width) +
    facet_grid(~type) + 
    scale_y_continuous(breaks=c(0, 0.5, 1), limits=c(-0.2, 1.5)) +
    scale_color_brewer(palette="Dark2", drop=F)
}
```

```{r}
plot_coefficient_CI_data(coefficient_CI_data)
ggsave("figures/error_dist_CI_example.png", width=5, height=3)

plot_coefficient_CI_data(coefficient_CI_data %>% 
                           filter(CI_type=="parametric"),
                         0.25)
ggsave("figures/error_dist_CI_example_parametric_only.png", width=5, height=3)
```

Challenge Q: Why isn't the error on the intercept changed in the scaling error case?

This can result in CIs which aren't actually at the nominal confidence level! And since CIs are equivalent to t-tests in this setting, this can also increase false positive rates.
(Also equivalent to Bayesian CrIs.)


```{r}
num_points = 200
true_intercept = 0
true_slope = 0.
set.seed(0)
parametric_ci_data = data.frame(IV = rep(runif(num_points,  -1, 1), 2),
                                type = rep(c("IID Error", "Scaling Error"), each=num_points),
                                error = rep(rnorm(num_points, 0, 1), 2)) %>%
  mutate(DV = ifelse(
    type == "IID Error",
    true_slope*IV + error,
    true_slope*IV + 2*abs(IV)*error)) # error increases proportional to distance from 0 on the IV

```

```{r}
ggplot(parametric_ci_data,
       aes(x=IV, y=DV, color=type)) +
  geom_point(alpha=0.5) +
  geom_smooth(method="lm", se=T, color="black", 
              size=1.5, level=0.9999) + # inflating the confidence bands a bit
                                       # to show their distribution is similar
  scale_color_brewer(palette="Accent") +
  facet_wrap(~type) +
  guides(color=F)
```

```{r}
ggsave("figures/error_dist_null.png", width=5, height=3)
```


```{r}
error_dist_null_sample = function(num_points) {
  true_intercept = 0
  true_slope = 0
  # We'll sample only for the scaling error case, we know IID works 
  this_data = data.frame(IV = runif(num_points,  -1, 1),
                         error = rnorm(num_points, 0, 1)) %>%
    mutate(DV = true_slope*IV + 2*abs(IV)*error) # error increases proportional to distance from 0 on the IV

  coefficient_CI_data = ex2_lm_bootstrap_CIs(this_data,
                                             R=500) # take fewer bootstrap samples, to speed things up
  
  coefficient_CI_data = coefficient_CI_data %>%
    gather(variable, value) %>%
    separate(variable, c("parameter", "measurement"), extra="merge") %>%
    spread(measurement, value) %>%
    mutate(parametric_CI_low=estimate-1.96*SE,
           parametric_CI_hi=estimate+1.96*SE) %>%
    gather(CI_type, value, contains("CI")) %>%
    separate(CI_type, c("CI_type", "high_or_low"), extra="merge") %>%
    spread(high_or_low, value) %>%
    mutate(significant = sign(CI_hi) == sign(CI_low)) %>%
    select(parameter, CI_type, significant)
  return(list(coefficient_CI_data))
}
```

```{r}
num_simulations = 500
num_points = 200
set.seed(0)
noise_dist_simulation_results = replicate(num_simulations, error_dist_null_sample(num_points)) %>%
  bind_rows()
```

```{r}
ggplot(noise_dist_simulation_results, aes(x=CI_type, fill=significant)) +
  geom_bar(stat="count", color="black") +
  scale_fill_brewer(palette="Set1", direction=-1) + 
  facet_wrap(~parameter) +
  scale_y_continuous(breaks=c(0, 0.05*num_simulations, num_simulations),
                     labels=c("0%", expression(Nominal ~ alpha), "100%")) +
  labs(x="CI Type",
       y="Proportion significant") +
  geom_hline(yintercept=0.05*num_simulations, linetype=2)
```

```{r}
ggsave("figures/error_dist_proportion_significant.png", width=5, height=3)
```

```{r}
noise_dist_simulation_results %>%
  count(parameter, CI_type, significant) %>%
  mutate(prop=n/num_simulations)
```

False positive rate nearly triples for the parametric model!

### Density estimate conceptual plot

```{r}
density_similarity_conceptual_plot_data = expand.grid(x=seq(0, 4, 0.01),
                                                      y=seq(0, 4, 0.01)) %>%
  mutate(population_1 = exp(-((x-2)^2 +(y-3)^2)/8) * exp(-((x-2/y)^2 +(y-1/x)^2)/2), #These are definitely not proper distributions
         population_2 = exp(-((x)^2 +(y)^2)/8) * exp(-((x/2)^2 +(y/2-1/x)^2)/2)) %>% 
  gather(population, value, contains("population"))
  
```

```{r}
ggplot(density_similarity_conceptual_plot_data,
       aes(x=x, y=y, z=value, color=population)) +
  geom_contour(size=1, bins=8) +
  scale_color_brewer(palette="Dark2") +
  facet_wrap(~population) +
  labs(x = "Feature 1", y="Feature 2") +
  guides(color=F)
```

```{r}
ggsave("figures/conceptual_density_plot.png", width=5, height=3)
```

## Bootstrap resampling

### Demo
```{r}
num_points = 100
true_intercept = 0
true_slope = 1.
set.seed(2) # I p-hacked the shit out of this demo to make the ideas more clear
parametric_ci_data = data.frame(IV = rep(runif(num_points,  -1, 1), 2),
                                type = rep(c("IID Error", "Scaling Error"), each=num_points),
                                error = rep(rnorm(num_points, 0, 1), 2)) %>%
  mutate(DV = ifelse(
    type == "IID Error",
    true_slope*IV + error,
    true_slope*IV + 2*abs(IV)*error)) # error increases proportional to distance from 0 on the IV
```

```{r}
p = ggplot(parametric_ci_data,
       aes(x=IV, y=DV)) +
  geom_point(alpha=0.25) +
  geom_smooth(method="lm", se=F, color="black", 
              size=1.5) + 
  facet_wrap(~type) 
p
```
```{r}
ggsave("figures/bootstrap_demo_0.png", width=5, height=3)
```

```{r}
set.seed(15) # See above RE: p-hacking
samp_1_indices = sample(2:num_points, num_points, replace=T)
samp_1 = parametric_ci_data[c(samp_1_indices, samp_1_indices+num_points), ] # take the same rows from each type of data
set.seed(2) # See above RE: p-hacking
samp_2_indices = sample(1:num_points, num_points, replace=T)
samp_2 = parametric_ci_data[c(samp_2_indices, samp_2_indices+num_points), ]

many_samples_indices = sample(1:num_points, 8*num_points, replace=T)
many_samples = bind_rows(samp_1 %>%
                           mutate(sample=1), 
                         samp_2 %>% mutate(sample=2),
                         parametric_ci_data[c(many_samples_indices, many_samples_indices+num_points),] %>%
                           mutate(sample=rep(rep(3:10, each=num_points), 2)))
```

```{r}
p + 
  geom_point(data=samp_1,
             aes(color=NA), color="red", alpha=0.5) 

```

```{r}
ggsave("figures/bootstrap_demo_1.png", width=5, height=3)
```

```{r}
p + 
  geom_point(data=samp_1,
             aes(color=NA), color="red", alpha=0.5) +
  geom_smooth(data=samp_1,
              method="lm", se=F, color="red", 
              size=1.5) 

```

```{r}
ggsave("figures/bootstrap_demo_2.png", width=5, height=3)
```

```{r}
p + 
  geom_point(data=samp_2,
             aes(color=NA), color="red", alpha=0.5) 
```

```{r}
ggsave("figures/bootstrap_demo_3.png", width=5, height=3)
```


```{r}
p + 
  geom_point(data=samp_2,
             aes(color=NA), color="red", alpha=0.5) +
  geom_smooth(data=samp_2,
              method="lm", se=F, color="red", 
              size=1.5) 
```

```{r}
ggsave("figures/bootstrap_demo_4.png", width=5, height=3)
```

```{r}
p +  
  geom_smooth(data=many_samples,
              aes(group=sample),
              method="lm", se=F, 
              size=1.5,
              color="red")
```


```{r}
ggsave("figures/bootstrap_demo_5.png", width=5, height=3)
```

# Applications

## Bootstrap confidence intervals




```{r}
num_top_points = 23
num_mid_points = 4
num_outlier_points = 2
max_score = 100
set.seed(0)
test_score_data = data.frame(
  score=c(rbinom(num_top_points, max_score, 0.9999), rbinom(num_mid_points, max_score, 0.97), sample(0:max_score, num_outlier_points, replace=T)),
  type="Observed sample"
)
```

```{r}
get_mean_score = function(data, indices) {
  return(mean(data[indices,]$score))
}

bootstrap_results = boot(test_score_data, get_mean_score, R=1000)
bootstrap_CIs = boot.ci(bootstrap_results)
bootstrap_CIs
```

```{r}
test_summary_data = test_score_data %>%
  summarise(mean=mean(score),
            se=sd(score)/sqrt(n()),
            parametric_CI_low=mean-1.96*se,
            parametric_CI_high=mean+1.96*se) 

test_score_data = test_score_data %>%
  bind_rows(
    data.frame(score=bootstrap_results$t, type="Boot. sampling dist.")
  )  %>%
  mutate(type=factor(type, levels=c("Observed sample", "Boot. sampling dist.")))

test_summary_data = test_summary_data %>%
  mutate(type = factor("Boot. sampling dist.", levels=levels(test_score_data$type)),
         percentile_CI_low=bootstrap_CIs$percent[4],
         percentile_CI_high=bootstrap_CIs$percent[5],
         bca_CI_low=bootstrap_CIs$bca[4],
         bca_CI_high=bootstrap_CIs$bca[5]) %>%
  gather(CI_type, value, contains("CI")) %>%
  separate(CI_type, c("CI_type", "endpoint"), extra="merge") %>%
  spread(endpoint, value) %>%
  mutate(y=c(170, 210, 190),
         CI_type=factor(CI_type, levels=c("parametric", "percentile", "bca"), labels=c("Normal", "Boot: %", "Boot: BCA")))
```

```{r}
ggplot(test_summary_data %>%
             filter(CI_type != "Boot: BCA"), aes(x=score)) +
  geom_histogram(data=test_score_data,
                 binwidth=1) +
  geom_point(mapping=aes(x=mean, y=y, color=CI_type),
             size=2) +
  geom_errorbarh(mapping=aes(y=y, color=CI_type, xmin=CI_low, x=NULL, xmax=CI_high),
                 size=1,
                 position=position_dodge()) +
  facet_grid(type ~ ., scales="free_y")+
  scale_color_brewer(palette="Dark2") +
  guides(color=guide_legend(title="CI"))


```
```{r}
ggsave("figures/bootstrap_CI_0.png", width=5, height=3)
```
```{r}
ggplot(test_summary_data, aes(x=score)) +
  geom_histogram(data=test_score_data,
                 binwidth=1) +
  geom_point(mapping=aes(x=mean, y=y, color=CI_type),
             size=2) +
  geom_errorbarh(mapping=aes(y=y, color=CI_type, xmin=CI_low, x=NULL, xmax=CI_high),
                 size=1,
                 position=position_dodge()) +
  facet_grid(type ~ ., scales="free_y")+
  scale_color_brewer(palette="Dark2") +
  guides(color=guide_legend(title="CI"))
ggsave("figures/bootstrap_CI_1.png", width=5, height=3)
```

## Bootstrap (& permutation) hypothesis tests

```{r}
num_flips = 20
true_heads_prob = 0.9
set.seed(2)
flips = rbinom(num_flips, 1, true_heads_prob)
flip_data = data.frame(flip_result = factor(flips, labels=c("Tails", "Heads")))
```

```{r}
flip_data_plot = ggplot(data = flip_data, aes(x = flips, fill = flips)) +
  geom_dotplot(binwidth=0.03) +
  scale_x_continuous(breaks = c(0, 1),
                     labels = c("tails", "heads")) +
  scale_y_continuous(breaks=c()) +
  labs(x="Flip result", y="")
```

```{r}
get_mean_heads = function(data, indices) {
  return(mean(data[indices, "flip_result"] == "Heads"))
}

set.seed(0)
bootstrap_results = boot(flip_data, get_mean_heads, R=20000)
bootstrap_CIs = boot.ci(bootstrap_results)
bootstrap_CIs
```

```{r}
flip_boot_plot = ggplot(data=data.frame(mean_flips=bootstrap_results$t),
                        aes(x=mean_flips)) +
  geom_histogram(binwidth=0.05) +
  xlim(0, 1) +
  geom_vline(xintercept=0.5,
             color="red",
             size=1.1) +
  annotate("text",
           label="Null value",
           color="red",
           x=0.43, y=2500,
           angle=90,
           size=4) +
  annotate("text",
           label="95% CI",
           color="blue",
           x=0.75, y=5400,
           size=4) +
  geom_errorbarh(aes(xmin=bootstrap_CIs$bca[4],
                    xmax=bootstrap_CIs$bca[5],
                    y=5100),
                 color="blue", 
                 size=1.1,
                 height=200) +
  labs(x="Boot. sampling dist.")

flip_boot_plot
```
```{r}
flip_data_plot + flip_boot_plot +
  plot_layout()


ggsave("figures/bootstrap_test.png", width=5, height=2.5)
```

```{r}
bootstrap_CIs = boot.ci(bootstrap_results,
                        conf=0.999)
flip_boot_plot = ggplot(data=data.frame(mean_flips=bootstrap_results$t),
                        aes(x=mean_flips)) +
  geom_histogram(binwidth=0.05) +
  xlim(0, 1) +
  geom_vline(xintercept=0.5,
             color="red",
             size=1.1) +
  annotate("text",
           label="Null value",
           color="red",
           x=0.43, y=2500,
           angle=90,
           size=4) +
  annotate("text",
           label="99.9% CI",
           color="blue",
           x=0.75, y=5400,
           size=4) +
  geom_errorbarh(aes(xmin=bootstrap_CIs$bca[4],
                    xmax=bootstrap_CIs$bca[5],
                    y=5100),
                 color="blue", 
                 size=1.1,
                 height=200) +
  labs(x="Boot. sampling dist.")

flip_boot_plot
```

```{r}
flip_data_plot + flip_boot_plot +
  plot_layout()


ggsave("figures/bootstrap_test_999.png", width=5, height=2.5)
```

## Bootstrap power analysis
