---
title: "Bootstrapping Lecture"
author: "Andrew Lampinen"
output:
  html_document:
    toc: true
    toc_float: true
---

```{r}
library(tidyverse)
library(boot)
```

```{r}
theme_set(theme_classic())
```

# What is bootstrapping anyway?

## What's wrong with parametric tests?

### T-tests on non-normal distributions

Let's see some examples! One common non-normal distribution is the *log-normal* distribution, i.e. a distribution that is normal after you take its logarithm. Many natural processes have distributions like this. One of particular interest to us is reaction times.

```{r}
parametric_plotting_data = data.frame(group = rep(rep(c("Group 1", "Group 2"), each=10000), 2),
                                      type = rep(c("Normal", "Log-normal"), each=20000),
                                      value = c(rnorm(10000, 0, 1),  # normal 1
                                                rnorm(10000, 0.66, 1),  # normal 2
                                                exp(rnorm(10000, 0, 1)),  # Non-normal 1
                                                exp(rnorm(10000, 0.66, 1))))  # Non-normal 2

```

Let's see how violating the assumption of normality changes the results of `t.test`. We'll compare two situations 

Valid: comparing two normally distributed populations with equal means but unequal variances.

Invalid: comparing two log-normally distributed populations with equal means but unequal variances.

```{r}
ggplot(parametric_plotting_data, aes(x=value, color=type)) +
  geom_density() +
  geom_vline(data=parametric_plotting_data %>%
               group_by(group, type) %>%
               summarize(mean_value = mean(value), sd_value = sd(value)),
             aes(xintercept=mean_value, color=type), linetype=2) +
  facet_grid(group ~ type, scales="free")
```

```{r}
gen_data_and_test = function(num_observations_per) {
x = rnorm(num_observations_per, 0, 1.1)
y = rnorm(num_observations_per, 1, 1.1)

pnormal = t.test(x, y)$p.value

# what if the data are log-normally distributed?
x = exp(rnorm(num_observations_per, 0, 1.1))
y = exp(rnorm(num_observations_per, 1, 1.1))

pnotnormal = t.test(x, y)$p.value 
return(c(pnormal, pnotnormal))
}

parametric_issues_demo = function(num_tests, num_observations_per) {
  replicate(num_tests, gen_data_and_test(num_observations_per))
}
```

```{r}
set.seed(0) # ensures we get the same results each time we run it

num_tests = 10000 # how many datasets to generate/tests to run
num_observations_per = 20 # how many obsservations in each dataset

parametric_issues_results = parametric_issues_demo(num_tests=num_tests, 
                                                   num_observations_per=num_observations_per)




parametric_issues_d = data.frame(valid_tests = parametric_issues_results[1,],
                                 invalid_tests = parametric_issues_results[2,],
                                 iteration=1:num_tests) %>%
  gather(type, p_value, contains("tests")) %>%
  mutate(is_significant = p_value < 0.05)

# Number significant results with normally distributed data
sum(parametric_issues_results[1,] < 0.05)

# number of significant results with log-normally distributed data
sum(parametric_issues_results[2,] < 0.05)
```
 
```{r}
ggplot(parametric_issues_d, aes(x=type, fill=is_significant)) +
  geom_bar(stat="count", color="black") +
  scale_fill_brewer(palette="Set1") +
  labs(title="Parametric t-test")
```

That's a non-trivial reduction in power from a misspecified model! (~80% to ~54%).

```{r}
boot_mean_diff_test = function(x, y) {
  boot_iterate = function(x, y) {
    x_samp = sample(x, 
                    length(x), 
                    replace=T)
    y_samp = sample(y, 
                    length(y), 
                    replace=T)
    mean_diff = mean(y_samp) - mean(x_samp)
    return(mean_diff)
  }
  boots = replicate(100, boot_iterate(x, y))
  quants = quantile(boots, probs=c(0.025, 0.925))
  return(sign(quants[1]) == sign(quants[2]))

}
```


```{r}
gen_data_and_boot_test = function(num_observations_per) {
x = rnorm(num_observations_per, 0, 1.1)
y = rnorm(num_observations_per, 1, 1.1)

pnormal = boot_mean_diff_test(x, y)

# what if the data are log-normally distributed?
x = exp(rnorm(num_observations_per, 0, 1.1))
y = exp(rnorm(num_observations_per, 1, 1.1))

pnotnormal = boot_mean_diff_test(x, y)
return(c(pnormal, pnotnormal))
}

boot_results = replicate(num_tests, gen_data_and_boot_test(num_observations_per))
sum(boot_results[1,])
sum(boot_results[2,])
```

While the bootstrap **actually loses power** relative to a perfectly specified model, it is much more **robust** to changes in the assumptions of that model, and so it **retains more power when assumptions are violated**.

### Non-IID noise and linear models

```{r}
num_points = 500
true_intercept = 0
true_slope = 1.
set.seed(0)
parametric_ci_data = data.frame(IV = rep(runif(num_points,  -1, 1), 2),
                                type = rep(c("IID Error", "Scaling Error"), each=num_points),
                                error = rep(rnorm(num_points, 0, 1), 2)) %>%
  mutate(DV = ifelse(
    type == "IID Error",
    true_slope*IV + error,
    true_slope*IV + 2*abs(IV)*error)) # error increases proportional to distance from 0 on the IV

```

```{r}
ggplot(parametric_ci_data,
       aes(x=IV, y=DV, color=type)) +
  geom_point(alpha=0.5) +
  geom_smooth(method="lm", se=T, color="black", 
              size=1.5, level=0.9999) + # inflating the confidence bands a bit
                                       # to show their distribution is similar
  scale_color_brewer(palette="Accent") +
  facet_wrap(~type) +
  guides(color=F)
```
```{r}
ggsave("figures/error_dist_non_null.png", width=5, height=3)
```

C.f. Anscombe's quartet, etc.

```{r}
summary(lm(DV ~ IV, parametric_ci_data %>% filter(type=="IID Error")))
summary(lm(DV ~ IV, parametric_ci_data %>% filter(type=="Scaling Error")))
```
```{r}
ex2_lm_bootstrap_CIs = function(data, R=1000) {
  lm_results = summary(lm(DV ~ IV, data=data))$coefficients
  bootstrap_coefficients = function(data, indices) {
    linear_model = lm(DV ~ IV, 
                      data=data[indices,]) # will select a bootstrap sample of the data
    return(linear_model$coefficients)
  }

  boot_results = boot(data=data,
                      statistic=bootstrap_coefficients,
                      R=R)
  boot_intercept_CI = boot.ci(boot_results, index=1, type="bca")
  boot_slope_CI = boot.ci(boot_results, index=2, type="bca")
  return(data.frame(intercept_estimate=lm_results[1, 1],
                    intercept_SE=lm_results[1,2],
                    slope_estimate=lm_results[2, 1],
                    slope_SE=lm_results[2,2],
                    intercept_boot_CI_low=boot_intercept_CI$bca[4],
                    intercept_boot_CI_hi=boot_intercept_CI$bca[5],
                    slope_boot_CI_low=boot_slope_CI$bca[4],
                    slope_boot_CI_hi=boot_slope_CI$bca[5]))
}
```

```{r}
set.seed(0) # for bootstraps
coefficient_CI_data = parametric_ci_data %>%
  group_by(type) %>%
  do(ex2_lm_bootstrap_CIs(.)) %>%
  ungroup() 
```

```{r}
coefficient_CI_data = coefficient_CI_data %>%
  gather(variable, value, -type) %>%
  separate(variable, c("parameter", "measurement"), extra="merge") %>%
  spread(measurement, value) %>%
  mutate(parametric_CI_low=estimate-1.96*SE,
         parametric_CI_hi=estimate+1.96*SE) %>%
  gather(CI_type, value, contains("CI")) %>%
  separate(CI_type, c("CI_type", "high_or_low"), extra="merge") %>%
  spread(high_or_low, value)
```

```{r}
ggplot(data=coefficient_CI_data, aes(x=parameter, color=CI_type, y=estimate, ymin=CI_low, ymax=CI_hi, linetype=CI_type)) +
  geom_hline(data=data.frame(parameter=c("intercept", "slope"),
                             estimate=c(true_intercept, true_slope)), 
             mapping=aes(yintercept=estimate),
             size=1, linetype=3) +
  geom_point(size=5, position=position_dodge(width=0.15)) +
  geom_errorbar(position=position_dodge(width=0.15), size=1) +
  facet_grid(~type) + 
  scale_y_continuous(breaks=c(0, 0.5, 1)) +
  scale_color_brewer(palette="Dark2")
```
```{r}
ggsave("figures/error_dist_CI_example.png", width=5, height=3)
```

Challenge Q: Why isn't the error on the intercept changed in the scaling error case?

This can result in CIs which aren't actually at the nominal confidence level! And since CIs are equivalent to t-tests in this setting, this can also increase false positive rates.
(Also equivalent to Bayesian CrIs.)


```{r}
num_points = 200
true_intercept = 0
true_slope = 0.
set.seed(0)
parametric_ci_data = data.frame(IV = rep(runif(num_points,  -1, 1), 2),
                                type = rep(c("IID Error", "Scaling Error"), each=num_points),
                                error = rep(rnorm(num_points, 0, 1), 2)) %>%
  mutate(DV = ifelse(
    type == "IID Error",
    true_slope*IV + error,
    true_slope*IV + 2*abs(IV)*error)) # error increases proportional to distance from 0 on the IV

```

```{r}
ggplot(parametric_ci_data,
       aes(x=IV, y=DV, color=type)) +
  geom_point(alpha=0.5) +
  geom_smooth(method="lm", se=T, color="black", 
              size=1.5, level=0.9999) + # inflating the confidence bands a bit
                                       # to show their distribution is similar
  scale_color_brewer(palette="Accent") +
  facet_wrap(~type) +
  guides(color=F)
```

```{r}
ggsave("figures/error_dist_null.png", width=5, height=3)
```


```{r}
error_dist_null_sample = function(num_points) {
  true_intercept = 0
  true_slope = 0
  # We'll sample only for the scaling error case, we know IID works 
  this_data = data.frame(IV = runif(num_points,  -1, 1),
                         error = rnorm(num_points, 0, 1)) %>%
    mutate(DV = true_slope*IV + 2*abs(IV)*error) # error increases proportional to distance from 0 on the IV

  coefficient_CI_data = ex2_lm_bootstrap_CIs(this_data,
                                             R=500) # take fewer bootstrap samples, to speed things up
  
  coefficient_CI_data = coefficient_CI_data %>%
    gather(variable, value) %>%
    separate(variable, c("parameter", "measurement"), extra="merge") %>%
    spread(measurement, value) %>%
    mutate(parametric_CI_low=estimate-1.96*SE,
           parametric_CI_hi=estimate+1.96*SE) %>%
    gather(CI_type, value, contains("CI")) %>%
    separate(CI_type, c("CI_type", "high_or_low"), extra="merge") %>%
    spread(high_or_low, value) %>%
    mutate(significant = sign(CI_hi) == sign(CI_low)) %>%
    select(parameter, CI_type, significant)
  return(list(coefficient_CI_data))
}
```

```{r}
num_simulations = 500
num_points = 200
set.seed(0)
noise_dist_simulation_results = replicate(num_simulations, error_dist_null_sample(num_points)) %>%
  bind_rows()
```

```{r}
ggplot(noise_dist_simulation_results, aes(x=CI_type, fill=significant)) +
  geom_bar(stat="count", color="black") +
  scale_fill_brewer(palette="Set1", direction=-1) + 
  facet_wrap(~parameter) +
  scale_y_continuous(breaks=c(0, 0.05*num_simulations, num_simulations),
                     labels=c("0%", expression(Nominal ~ alpha), "100%")) +
  labs(x="CI Type",
       y="Proportion significant") +
  geom_hline(yintercept=0.05*num_simulations, linetype=2)
```
```{r}
ggsave("figures/error_dist_proportion_significant.png", width=5, height=3)
```

```{r}
noise_dist_simulation_results %>%
  count(parameter, CI_type, significant) %>%
  mutate(prop=n/num_simulations)
```

Alpha nearly triples for the parametric model!

# Applications

## Bootstrap confidence intervals

## Bootstrap (& permutation) hypothesis tests

## Bootstrap power analysis